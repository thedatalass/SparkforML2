{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Spark\n",
    "Apache Spark is an open-source powerful distributed querying and processing\n",
    "engine. It provides flexibility and extensibility of MapReduce but at significantly\n",
    "higher speeds: Up to 100 times faster than Apache Hadoop when data is stored in\n",
    "memory and up to 10 times when accessing disk\n",
    "\n",
    "\n",
    "### Spark Ecosystem\n",
    "Other than Spark Core API, there are additional libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics and Machine Learning areas.\n",
    "\n",
    "These libraries include:\n",
    "\n",
    "* Spark Streaming:\n",
    "\n",
    "* Spark SQL:\n",
    "\n",
    "* Spark MLlib:\n",
    "\n",
    "* Spark GraphX:\n",
    "\n",
    "\n",
    "![spark ecosystem](ecosystem.png)\n",
    "\n",
    "### Spark Architecture\n",
    "Spark Architecture includes following three main components:\n",
    "\n",
    "* Data Storage\n",
    "* API\n",
    "* Management Framework\n",
    "\n",
    "\n",
    "Spark can be deployed as a Stand-alone server or it can be on a distributed computing framework like Mesos or YARN.\n",
    "\n",
    "![spark architecture](architecture.png)\n",
    "\n",
    "### Spark running modes\n",
    "When you install Spark on the local machine or use a Cloud based installation, there are few different modes you can connect to Spark engine.\n",
    "\n",
    "The following table shows the Master URL parameter for the different modes of running Spark.\n",
    "![spark modes](modes.png)\n",
    "\n",
    "### Spark Web Console\n",
    "When Spark is running in any mode, you can view the Spark job results and other statistics by accessing Spark Web Console via the following URL:\n",
    "\n",
    "http://localhost:4040\n",
    "\n",
    "\n",
    "### Spark Architecture\n",
    "Spark applications run as independent sets of processes on a cluster as described in the below diagram\n",
    "![spark cluster](cluster.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Resilient Distributed Dataset\n",
    "\n",
    "\n",
    "RDDs support two kinds of operations:\n",
    "\n",
    "* Transformation \n",
    "* Action \n",
    "\n",
    "\n",
    "### Spark Machine Learning library\n",
    "The Spark MLlib module offers machine learning functionality over a number of\n",
    "domains. The documentation available at the Spark website introduces the data\n",
    "types used (for example, vectors and the LabeledPoint structure). This module\n",
    "offers functionality that includes:\n",
    "* Statistics\n",
    "* Classification\n",
    "* Regression\n",
    "* Collaborative Filtering\n",
    "* Clustering\n",
    "* Dimensionality Reduction\n",
    "* Feature Extraction\n",
    "* Frequent Pattern Mining\n",
    "* Optimization\n",
    "\n",
    "### Performance issues\n",
    "1. The cluster structure \n",
    "2. The Hadoop file system \n",
    "3. Data locality\n",
    "4. Coding \n",
    "5. Cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First example \n",
    "\n",
    "Test if spark is running properly. We have to install findspark before we can run this code in cmd call:  \n",
    ">pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master='local[2]',appName='my-spark') #use 2 cores during session #name for context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',\n",
       " 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',\n",
       " 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',\n",
       " 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1',\n",
       " 'Hornet 4 Drive,21.4,6,258,110,3.08,3.215,19.44,1,0,3,1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the data\n",
    "rdd = sc.textFile('mtcars.csv') #Get the data from csv to rdd\n",
    "rdd.take(5) #take first 5 csv rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=sc.parallelize([1,2,3]) #Convert list to RDD\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to start Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two entry points to Spark\n",
    "\n",
    "With the release of Spark 2.0.0 there is a new abstraction available to developers - the Spark Session - which can be instantiated and called upon just like the Spark Context that was previously available. The Spark Session encapsulates the existing Spark Context therefore existing functionality should not be affected and developers may continue using the Spark Context as desired. However the new Spark Session abstraction is preferred by the Spark community in Spark 2.0.0 on beyond therefore we will be using this in the future.\n",
    "\n",
    "* **SparkContext**: create *RDD* and broadcast variables on the cluster.\n",
    "* **SparkSession**: create *DataFrame* (pyspark.sql.dataframe.DataFrame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop a running SparkContext before opening the new one\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The new way to get a spark entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe by loading file\n",
    "df=spark.read.csv('mtcars.csv',header=True,inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x1 x2\n",
      "0   1  a\n",
      "1   2  b\n",
      "2   3  c\n",
      "3   4  d\n",
      "4   5  e\n"
     ]
    }
   ],
   "source": [
    "#Another way to create dataframe\n",
    "import pandas as pd\n",
    "pdf=pd.DataFrame({\n",
    "    'x1':range(1,6),\n",
    "    'x2': list('abcde')\n",
    "})\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| x1| x2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  c|\n",
      "|  4|  d|\n",
      "|  5|  e|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD object\n",
    "\n",
    "\n",
    "\n",
    "The class pyspark.SparkContext creates a client which connects to a Spark cluster. This client can be used to create an RDD object. There are two methods from this class for directly creating RDD objects:\n",
    "* `parallelize()`\n",
    "* `textFile()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `parallelize()`\n",
    "\n",
    "`parallelize()` distribute a local **python collection** for form an RDD. Common built-in python collections include `dist`, `list`, `tuple` or `set`.\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'fish']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(('cat','dog','fish'))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `textFile()`\n",
    "\n",
    "The `textFile()` function reads a text file and returns it as an **RDD of strings**. Usually, you will need to apply some **map** functions to transform each elements of the RDD to some data structure/type that is suitable for data analysis.\n",
    "\n",
    "**When using `textFile()`, each line of the text file becomes an element in the resulting RDD.**\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fresh install of XP on new computer. Sweet relief! fuck vista\\t1018769417\\t1.0',\n",
       " 'Well. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\\t10284216536\\t1.0',\n",
       " '\"Literally six weeks before I can take off \"\"SSC Chair\"\" off my email. Its like the torturous 4th mile before everything stops hurting.\"\\t10298589026\\t1.0',\n",
       " 'Mitsubishi i MiEV - Wikipedia, the free encyclopedia - http://goo.gl/xipe Cutest car ever!\\t109017669432377344\\t1.0',\n",
       " \"'Cheap Eats in SLP' - http://t.co/4w8gRp7\\t109642968603963392\\t1.0\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.textFile('twitter.txt')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used functions with RDD objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions\n",
    "These functions are probably the most commonly used functions when dealing with an RDD object. \n",
    "\n",
    "* `map()`\n",
    "* `mapValues()`\n",
    "* `flatMap()`\n",
    "* `flatMapValues()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map()`\n",
    "The map() applies a function to each elements of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',\n",
       " 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',\n",
       " 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',\n",
       " 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1',\n",
       " 'Hornet 4 Drive,21.4,6,258,110,3.08,3.215,19.44,1,0,3,1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.textFile('mtcars.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  'mpg',\n",
       "  'cyl',\n",
       "  'disp',\n",
       "  'hp',\n",
       "  'drat',\n",
       "  'wt',\n",
       "  'qsec',\n",
       "  'vs',\n",
       "  'am',\n",
       "  'gear',\n",
       "  'carb'],\n",
       " ['Mazda RX4',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.62',\n",
       "  '16.46',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4'],\n",
       " ['Mazda RX4 Wag',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.875',\n",
       "  '17.02',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4'],\n",
       " ['Datsun 710',\n",
       "  '22.8',\n",
       "  '4',\n",
       "  '108',\n",
       "  '93',\n",
       "  '3.85',\n",
       "  '2.32',\n",
       "  '18.61',\n",
       "  '1',\n",
       "  '1',\n",
       "  '4',\n",
       "  '1']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert csv to tuple with car model and list of values\n",
    "rdd1=rdd.map(lambda x: x.split(',')) #define end line function to split by pattern defined by column, get list of string values\n",
    "rdd1.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  ['mpg',\n",
       "   'cyl',\n",
       "   'disp',\n",
       "   'hp',\n",
       "   'drat',\n",
       "   'wt',\n",
       "   'qsec',\n",
       "   'vs',\n",
       "   'am',\n",
       "   'gear',\n",
       "   'carb']),\n",
       " ('Mazda RX4',\n",
       "  ['21', '6', '160', '110', '3.9', '2.62', '16.46', '0', '1', '4', '4']),\n",
       " ('Mazda RX4 Wag',\n",
       "  ['21', '6', '160', '110', '3.9', '2.875', '17.02', '0', '1', '4', '4']),\n",
       " ('Datsun 710',\n",
       "  ['22.8', '4', '108', '93', '3.85', '2.32', '18.61', '1', '1', '4', '1'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2=rdd1.map(lambda x: (x[0],x[1:]))\n",
    "rdd_2.take(4) #all numbers are strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mazda RX4',\n",
       "  ['21', '6', '160', '110', '3.9', '2.62', '16.46', '0', '1', '4', '4']),\n",
       " ('Mazda RX4 Wag',\n",
       "  ['21', '6', '160', '110', '3.9', '2.875', '17.02', '0', '1', '4', '4']),\n",
       " ('Datsun 710',\n",
       "  ['22.8', '4', '108', '93', '3.85', '2.32', '18.61', '1', '1', '4', '1']),\n",
       " ('Hornet 4 Drive',\n",
       "  ['21.4', '6', '258', '110', '3.08', '3.215', '19.44', '1', '0', '3', '1']),\n",
       " ('Hornet Sportabout',\n",
       "  ['18.7', '8', '360', '175', '3.15', '3.44', '17.02', '0', '0', '3', '2'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first element of the csv file, the header\n",
    "rdd_temp=rdd_2.filter(lambda x: x[0] !='')\n",
    "rdd_temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mazda RX4',\n",
       "  [21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0]),\n",
       " ('Mazda RX4 Wag',\n",
       "  [21.0, 6.0, 160.0, 110.0, 3.9, 2.875, 17.02, 0.0, 1.0, 4.0, 4.0]),\n",
       " ('Datsun 710',\n",
       "  [22.8, 4.0, 108.0, 93.0, 3.85, 2.32, 18.61, 1.0, 1.0, 4.0, 1.0]),\n",
       " ('Hornet 4 Drive',\n",
       "  [21.4, 6.0, 258.0, 110.0, 3.08, 3.215, 19.44, 1.0, 0.0, 3.0, 1.0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each element is a tuple then a list. Convert all strings to lists\n",
    "rdd3=rdd_temp.map(lambda x: (x[0],[*map(float,x[1])]))\n",
    "rdd3.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mapValues()`\n",
    "\n",
    "This map function requires that each element in the RDD has a **key/value** pair structure, for example, a tuple of 2 items, or a list of 2 items.\n",
    "\n",
    "The RDD object **rdd_temp** and **rdd_3** belong to this category. If we only want to operate on the values, we can use the `mapValues()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mazda RX4',\n",
       "  ['21', '6', '160', '110', '3.9', '2.62', '16.46', '0', '1', '4', '4']),\n",
       " ('Mazda RX4 Wag',\n",
       "  ['21', '6', '160', '110', '3.9', '2.875', '17.02', '0', '1', '4', '4']),\n",
       " ('Datsun 710',\n",
       "  ['22.8', '4', '108', '93', '3.85', '2.32', '18.61', '1', '1', '4', '1']),\n",
       " ('Hornet 4 Drive',\n",
       "  ['21.4', '6', '258', '110', '3.08', '3.215', '19.44', '1', '0', '3', '1'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_temp.take(4) #Mazda RX4 is key and then values are stored in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mazda RX4',\n",
       "  [21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0]),\n",
       " ('Mazda RX4 Wag',\n",
       "  [21.0, 6.0, 160.0, 110.0, 3.9, 2.875, 17.02, 0.0, 1.0, 4.0, 4.0]),\n",
       " ('Datsun 710',\n",
       "  [22.8, 4.0, 108.0, 93.0, 3.85, 2.32, 18.61, 1.0, 1.0, 4.0, 1.0]),\n",
       " ('Hornet 4 Drive',\n",
       "  [21.4, 6.0, 258.0, 110.0, 3.08, 3.215, 19.44, 1.0, 0.0, 3.0, 1.0]),\n",
       " ('Hornet Sportabout',\n",
       "  [18.7, 8.0, 360.0, 175.0, 3.15, 3.44, 17.02, 0.0, 0.0, 3.0, 2.0])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert all list elements to numbers\n",
    "rdd_mapValues=rdd_temp.mapValues(lambda x: [*map(float,x)])\n",
    "rdd_mapValues.take(5)\n",
    "#only works for key values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `flatMap()`\n",
    "\n",
    "This function **first** applies a function to each elements of an RDD and **then** flatten the results. We can simply use this function to flatten elements of an RDD without extra operation on each elements.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'a'), ('b', 'b')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([('a','a'),('b','b')])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b', 'b']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x).collect() #every element inside 1 list - not saving results so use 'collect'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `flatMapValues()`\n",
    "\n",
    "This function implements the `flatMap` function on the value for each **key/value** pair elements. It applies a function only to the value of each **key/value** pairs and then flatten the results. \n",
    "\n",
    "A good use case is to use this function to **\"melt\"** a data frame, like the `melt()` function from the R package `reshape2`. To better explain this idea, we create a data frame with the **SparkSession** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ozone</th>\n",
       "      <th>solar.r</th>\n",
       "      <th>wind</th>\n",
       "      <th>temp</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ozone  solar.r  wind  temp  month  day\n",
       "0   41.0    190.0   7.4    67      5    1\n",
       "1   36.0    118.0   8.0    72      5    2\n",
       "2   12.0    149.0  12.6    74      5    3\n",
       "3   18.0    313.0  11.5    62      5    4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv('airquality.csv',inferSchema=True,header=True,nullValue='NA')\n",
    "df.toPandas().iloc[:4,] #convert df to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air=df.rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 1), (41, 190, 7.4, 67)],\n",
       " [(5, 2), (36, 118, 8.0, 72)],\n",
       " [(5, 3), (12, 149, 12.6, 74)],\n",
       " [(5, 4), (18, 313, 11.5, 62)],\n",
       " [(5, 5), (None, None, 14.3, 56)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine month and day into a tuple and the remaining values into a 2nd tuple\n",
    "air1=air.map(lambda x: [x[4:],x[:4]])\n",
    "air1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((5, 1), [('ozone', 41), ('solar.r', 190), ('wind', 7.4), ('temp', 67)]),\n",
       " ((5, 2), [('ozone', 36), ('solar.r', 118), ('wind', 8.0), ('temp', 72)]),\n",
       " ((5, 3), [('ozone', 12), ('solar.r', 149), ('wind', 12.6), ('temp', 74)]),\n",
       " ((5, 4), [('ozone', 18), ('solar.r', 313), ('wind', 11.5), ('temp', 62)]),\n",
       " ((5, 5), [('ozone', None), ('solar.r', None), ('wind', 14.3), ('temp', 56)])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air2=air1.mapValues(lambda x: [('ozone',x[0]),('solar.r', x[1]), ('wind',x[2]), ('temp',x[3])])\n",
    "air2.take(5) #associate numeric values with a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((5, 1), ('ozone', 41)),\n",
       " ((5, 1), ('solar.r', 190)),\n",
       " ((5, 1), ('wind', 7.4)),\n",
       " ((5, 1), ('temp', 67))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air3=air2.flatMapValues(lambda x: x)\n",
    "air3.take(4) #data format is (month,day) then (weather variable,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate functions\n",
    "Two aggregate functions:\n",
    "\n",
    "* `aggregate()`\n",
    "* `aggregateByKey()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `aggregate(zeroValue, seqOp, combOp)`\n",
    "\n",
    "* **zeroValue** is like a data container. Its structure should match with the data structure of the returned values from the seqOp function.\n",
    "* **seqOp** is a function that takes two arguments: the first argument is the zeroValue and the second argument is an element from the RDD. The zeroValue gets updated with the returned value after every run.\n",
    "* **combOp** is a function that takes two arguments: the first argument is the final zeroValue from one partition and the other is another final zeroValue from another partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=22.8, disp=108.0),\n",
       " Row(mpg=21.4, disp=258.0),\n",
       " Row(mpg=18.7, disp=360.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df=spark.read.csv('mtcars.csv',inferSchema=True, header=True).select(['mpg','disp'])\n",
    "mtcars_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.090625000000003 230.721875\n"
     ]
    }
   ],
   "source": [
    "#calculate average\n",
    "mpg_mean = mtcars_df.select('mpg').rdd.map(lambda x: x[0]).mean()\n",
    "disp_mean = mtcars_df.select('disp').rdd.map(lambda x: x[0]).mean()\n",
    "print(mpg_mean,disp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126.0471874999998, 476184.7946875)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroValue =(0,0)\n",
    "seqOp=lambda z, x: (z[0]+(x[0]-mpg_mean)**2, z[1]+(x[1]-disp_mean)**2) #z is zeroValue, x is element in RDD partition\n",
    "#sum of squares calculation, values updated after every row in the dataframe\n",
    "#function combines all zeroValues into 1\n",
    "comboOp= lambda px, py: (px[0]+py[0],px[1]+py[1])\n",
    "\n",
    "mtcars_df.rdd.aggregate(zeroValue, seqOp, comboOp)\n",
    "#'.rdd' converts from spark df to spark RDD\n",
    "#Output is sum of squares for mpg and disp values in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `aggregateByKey(zeroValue, seqOp, combOp)`\n",
    "\n",
    "This function does similar things as aggregate(). The aggregate() aggregate all results to the very end, but aggregateByKey() merge results by key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame object\n",
    "DataFrames, like RDDs, are immutable collections of data distributed among the\n",
    "nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into\n",
    "named columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame by reading a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame with `createDataFrame` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From RDD object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=[1, 2, 3], y=['a', 'b', 'c'])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df=sc.parallelize([\n",
    "   Row(x=[1,2,3],y=['a','b','c']) \n",
    "])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|        x|        y|\n",
      "+---------+---------+\n",
      "|[1, 2, 3]|[a, b, c]|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert to dataframe\n",
    "df_1=spark.createDataFrame(df)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     b|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create spark dataframe from a list\n",
    "my_list=[['a',1],['b',2]]\n",
    "df=spark.createDataFrame(my_list,['letter','number'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a list of tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on DataFrame columns\n",
    "\n",
    "Column instances can be created in two ways:\n",
    "\n",
    "1. directly select a column out of a *DataFrame*: `df.colName`\n",
    "2. create from a column expression: `df.colName + 1`\n",
    "\n",
    "The column classes come with some methods that can operate on a column instance. ***However, almost all functions from the `pyspark.sql.functions` module take one or more column instances as argument(s)***. These functions are important for data manipulation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame column methods\n",
    "\n",
    "### Methods that take column names as arguments:\n",
    "\n",
    "* `corr(col1, col2)`: two column names.\n",
    "* `cov(col1, col2)`: two column names.\n",
    "* `crosstab(col1, col2)`: two column names.\n",
    "* `describe(*cols)`: ***`*cols` refers to only column names (strings).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| mpg| disp|\n",
      "+----+-----+\n",
      "|21.0|160.0|\n",
      "|21.0|160.0|\n",
      "|22.8|108.0|\n",
      "|21.4|258.0|\n",
      "+----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.8475513792624786"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.show(4)\n",
    "mtcars_df.corr('mpg','disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods that take column names or column expressions or **both** as arguments:\n",
    "\n",
    "* `cube(*cols)`: column names (string) or column expressions or **both**.\n",
    "* `drop(*cols)`: ***a list of column names OR a single column expression.***\n",
    "* `groupBy(*cols)`: column name (string) or column expression or **both**.\n",
    "* `rollup(*cols)`: column name (string) or column expression or **both**.\n",
    "* `select(*cols)`: column name (string) or column expression or **both**.\n",
    "* `sort(*cols, **kwargs)`: column name (string) or column expression or **both**.\n",
    "* `sortWithinPartitions(*cols, **kwargs)`: column name (string) or column expression or **both**.\n",
    "* `orderBy(*cols, **kwargs)`: column name (string) or column expression or **both**.\n",
    "* `sampleBy(col, fractions, sed=None)`: a column name.\n",
    "* `toDF(*cols)`: **a list of column names (string).**\n",
    "* `withColumn(colName, col)`: `colName` refers to column name; `col` refers to a column expression.\n",
    "* `withColumnRenamed(existing, new)`: takes column names as arguments.\n",
    "* `filter(condition)`: ***condition** refers to a column expression that returns `types.BooleanType` of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| disp|\n",
      "+-----+\n",
      "|160.0|\n",
      "|160.0|\n",
      "|108.0|\n",
      "|258.0|\n",
      "|360.0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mtcars_df.drop('mpg').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion between RDD and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame to RDD\n",
    "A **DataFrame** can be easily converted to an **RDD** by calling the `pyspark.sql.DataFrame.rdd()` function. Each element in the returned RDD is an **pyspark.sql.Row** object. An Row is a list of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=22.8, disp=108.0),\n",
       " Row(mpg=21.4, disp=258.0),\n",
       " Row(mpg=18.7, disp=360.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.rdd.take(5) #each element is a row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21.0, 160.0), (21.0, 160.0), (22.8, 108.0), (21.4, 258.0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.rdd.map(lambda x: (x['mpg'],x['disp'])).take(4) #combine with map function to aggregate variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to DataFrame\n",
    "\n",
    "To convert an RDD to a DataFrame, we can use the `SparkSession.createDataFrame()` function. Every element in the RDD has to be a Row.\n",
    "\n",
    "Create an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RDD elements to RDD Row objects\n",
    "First we define a function which takes a list of column names and a list of values and create a Row of key-value pairs. Since keys in an Row object are variable names, we canâ€™t simply pass a dictionary to the Row() function. We can think of a dictionary as an argument list and use the ** to unpack the argument list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and split columns\n",
    "\n",
    "Sometimes we need to merge multiple columns in a Dataframe into one column, or split a column into multiple columns. We can easily achieve this by converting a DataFrame to RDD, applying map functions to manipulate elements, and then converting the RDD back to a DataFrame.\n",
    "\n",
    "#### Convert DataFrame to RDD and merge values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|           _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+--------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|     Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "| Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|    Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "+--------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the data\n",
    "mtcars=spark.read.csv('mtcars.csv',inferSchema=True, header=True)\n",
    "mtcars.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model='Mazda RX4', values=[21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0]),\n",
       " Row(model='Mazda RX4 Wag', values=[21.0, 6.0, 160.0, 110.0, 3.9, 2.875, 17.02, 0.0, 1.0, 4.0, 4.0]),\n",
       " Row(model='Datsun 710', values=[22.8, 4.0, 108.0, 93.0, 3.85, 2.32, 18.61, 1.0, 1.0, 4.0, 1.0]),\n",
       " Row(model='Hornet 4 Drive', values=[21.4, 6.0, 258.0, 110.0, 3.08, 3.215, 19.44, 1.0, 0.0, 3.0, 1.0]),\n",
       " Row(model='Hornet Sportabout', values=[18.7, 8.0, 360.0, 175.0, 3.15, 3.44, 17.02, 0.0, 0.0, 3.0, 2.0]),\n",
       " Row(model='Valiant', values=[18.1, 6.0, 225.0, 105.0, 2.76, 3.46, 20.22, 1.0, 0.0, 3.0, 1.0]),\n",
       " Row(model='Duster 360', values=[14.3, 8.0, 360.0, 245.0, 3.21, 3.57, 15.84, 0.0, 0.0, 3.0, 4.0]),\n",
       " Row(model='Merc 240D', values=[24.4, 4.0, 146.7, 62.0, 3.69, 3.19, 20.0, 1.0, 0.0, 4.0, 2.0]),\n",
       " Row(model='Merc 230', values=[22.8, 4.0, 140.8, 95.0, 3.92, 3.15, 22.9, 1.0, 0.0, 4.0, 2.0]),\n",
       " Row(model='Merc 280', values=[19.2, 6.0, 167.6, 123.0, 3.92, 3.44, 18.3, 1.0, 0.0, 4.0, 4.0]),\n",
       " Row(model='Merc 280C', values=[17.8, 6.0, 167.6, 123.0, 3.92, 3.44, 18.9, 1.0, 0.0, 4.0, 4.0]),\n",
       " Row(model='Merc 450SE', values=[16.4, 8.0, 275.8, 180.0, 3.07, 4.07, 17.4, 0.0, 0.0, 3.0, 3.0]),\n",
       " Row(model='Merc 450SL', values=[17.3, 8.0, 275.8, 180.0, 3.07, 3.73, 17.6, 0.0, 0.0, 3.0, 3.0]),\n",
       " Row(model='Merc 450SLC', values=[15.2, 8.0, 275.8, 180.0, 3.07, 3.78, 18.0, 0.0, 0.0, 3.0, 3.0]),\n",
       " Row(model='Cadillac Fleetwood', values=[10.4, 8.0, 472.0, 205.0, 2.93, 5.25, 17.98, 0.0, 0.0, 3.0, 4.0]),\n",
       " Row(model='Lincoln Continental', values=[10.4, 8.0, 460.0, 215.0, 3.0, 5.424, 17.82, 0.0, 0.0, 3.0, 4.0]),\n",
       " Row(model='Chrysler Imperial', values=[14.7, 8.0, 440.0, 230.0, 3.23, 5.345, 17.42, 0.0, 0.0, 3.0, 4.0]),\n",
       " Row(model='Fiat 128', values=[32.4, 4.0, 78.7, 66.0, 4.08, 2.2, 19.47, 1.0, 1.0, 4.0, 1.0]),\n",
       " Row(model='Honda Civic', values=[30.4, 4.0, 75.7, 52.0, 4.93, 1.615, 18.52, 1.0, 1.0, 4.0, 2.0]),\n",
       " Row(model='Toyota Corolla', values=[33.9, 4.0, 71.1, 65.0, 4.22, 1.835, 19.9, 1.0, 1.0, 4.0, 1.0]),\n",
       " Row(model='Toyota Corona', values=[21.5, 4.0, 120.1, 97.0, 3.7, 2.465, 20.01, 1.0, 0.0, 3.0, 1.0]),\n",
       " Row(model='Dodge Challenger', values=[15.5, 8.0, 318.0, 150.0, 2.76, 3.52, 16.87, 0.0, 0.0, 3.0, 2.0]),\n",
       " Row(model='AMC Javelin', values=[15.2, 8.0, 304.0, 150.0, 3.15, 3.435, 17.3, 0.0, 0.0, 3.0, 2.0]),\n",
       " Row(model='Camaro Z28', values=[13.3, 8.0, 350.0, 245.0, 3.73, 3.84, 15.41, 0.0, 0.0, 3.0, 4.0]),\n",
       " Row(model='Pontiac Firebird', values=[19.2, 8.0, 400.0, 175.0, 3.08, 3.845, 17.05, 0.0, 0.0, 3.0, 2.0]),\n",
       " Row(model='Fiat X1-9', values=[27.3, 4.0, 79.0, 66.0, 4.08, 1.935, 18.9, 1.0, 1.0, 4.0, 1.0]),\n",
       " Row(model='Porsche 914-2', values=[26.0, 4.0, 120.3, 91.0, 4.43, 2.14, 16.7, 0.0, 1.0, 5.0, 2.0]),\n",
       " Row(model='Lotus Europa', values=[30.4, 4.0, 95.1, 113.0, 3.77, 1.513, 16.9, 1.0, 1.0, 5.0, 2.0]),\n",
       " Row(model='Ford Pantera L', values=[15.8, 8.0, 351.0, 264.0, 4.22, 3.17, 14.5, 0.0, 1.0, 5.0, 4.0]),\n",
       " Row(model='Ferrari Dino', values=[19.7, 6.0, 145.0, 175.0, 3.62, 2.77, 15.5, 0.0, 1.0, 5.0, 6.0]),\n",
       " Row(model='Maserati Bora', values=[15.0, 8.0, 301.0, 335.0, 3.54, 3.57, 14.6, 0.0, 1.0, 5.0, 8.0]),\n",
       " Row(model='Volvo 142E', values=[21.4, 4.0, 121.0, 109.0, 4.11, 2.78, 18.6, 1.0, 1.0, 4.0, 2.0])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert dataframe to RDD\n",
    "rdd_merged=mtcars.rdd.map(lambda x: Row(model=x[0], values=list(map(float, x[1:]))))\n",
    "rdd_merged.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RDD back to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------------------------------------------+\n",
      "|model            |values                                                           |\n",
      "+-----------------+-----------------------------------------------------------------+\n",
      "|Mazda RX4        |[21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0]  |\n",
      "|Mazda RX4 Wag    |[21.0, 6.0, 160.0, 110.0, 3.9, 2.875, 17.02, 0.0, 1.0, 4.0, 4.0] |\n",
      "|Datsun 710       |[22.8, 4.0, 108.0, 93.0, 3.85, 2.32, 18.61, 1.0, 1.0, 4.0, 1.0]  |\n",
      "|Hornet 4 Drive   |[21.4, 6.0, 258.0, 110.0, 3.08, 3.215, 19.44, 1.0, 0.0, 3.0, 1.0]|\n",
      "|Hornet Sportabout|[18.7, 8.0, 360.0, 175.0, 3.15, 3.44, 17.02, 0.0, 0.0, 3.0, 2.0] |\n",
      "+-----------------+-----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged=spark.createDataFrame(rdd_merged)\n",
    "df_merged.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the values column to two columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+\n",
      "| x1|    x2| x3| x4| y1| y2|\n",
      "+---+------+---+---+---+---+\n",
      "|  a| apple|  1|2.4|  1|yes|\n",
      "|  a|orange|  1|2.5|  0| no|\n",
      "|  b|orange|  2|3.5|  1| no|\n",
      "|  b|orange|  2|1.4|  0|yes|\n",
      "|  b| peach|  2|2.1|  0|yes|\n",
      "|  c| peach|  4|1.5|  1|yes|\n",
      "+---+------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create some example dataset\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'x1': ['a','a','b','b', 'b', 'c'],\n",
    "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
    "        'x3': [1, 1, 2, 2, 2, 4],\n",
    "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
    "        'y1': [1, 0, 1, 0, 0, 1],\n",
    "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping String Column to Index column with StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StringIndexer` maps a string column to a index column that will be treated as a categorical column by spark. **The indices start with 0 and are ordered by label frequencies**. If it is a numerical column, the column will first be casted to a string column and then indexed by  StringIndexer.\n",
    "\n",
    "There are three steps to implement the StringIndexer\n",
    "\n",
    "1. Build the StringIndexer model: specify the input column and output column names.\n",
    "2. Learn the StringIndexer model: fit the model with your data.\n",
    "3. Execute the indexing: call the transform function to execute the indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+----------+\n",
      "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
      "+---+------+---+---+---+---+----------+\n",
      "|  a| apple|  1|2.4|  1|yes|       1.0|\n",
      "|  a|orange|  1|2.5|  0| no|       1.0|\n",
      "|  b|orange|  2|3.5|  1| no|       0.0|\n",
      "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
      "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
      "|  c| peach|  4|1.5|  1|yes|       2.0|\n",
      "+---+------+---+---+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "s_indexer = StringIndexer(inputCol='x1',outputCol='indexed_x1')\n",
    "s_indexer_model=s_indexer.fit(df)\n",
    "df_s_indexer=s_indexer_model.transform(df)\n",
    "df_s_indexer.show()\n",
    "\n",
    "#B is most frequent which is why it has a value of 0 for 'indexed_x1' colun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x1', 'string'),\n",
       " ('x2', 'string'),\n",
       " ('x3', 'bigint'),\n",
       " ('x4', 'double'),\n",
       " ('y1', 'bigint'),\n",
       " ('y2', 'string'),\n",
       " ('indexed_x1', 'double')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the datatypes in the dataframe\n",
    "df_s_indexer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x1|\n",
      "+---+\n",
      "|  a|\n",
      "|  a|\n",
      "|  b|\n",
      "|  b|\n",
      "|  b|\n",
      "|  c|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "df_ohe=df.select('x1')\n",
    "df_ohe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| x1|indexed_x1|\n",
      "+---+----------+\n",
      "|  a|       1.0|\n",
      "|  a|       1.0|\n",
      "|  b|       0.0|\n",
      "|  b|       0.0|\n",
      "|  b|       0.0|\n",
      "|  c|       2.0|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#String index this categorical column\n",
    "df_x1_indexed = StringIndexer(inputCol='x1',outputCol='indexed_x1').fit(df_ohe).transform(df_ohe)\n",
    "df_x1_indexed.show() #If assigning output to a name, use separate line for show function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+\n",
      "| x1|indexed_x1|   encoded_x1|\n",
      "+---+----------+-------------+\n",
      "|  a|       1.0|(2,[1],[1.0])|\n",
      "|  a|       1.0|(2,[1],[1.0])|\n",
      "|  b|       0.0|(2,[0],[1.0])|\n",
      "|  b|       0.0|(2,[0],[1.0])|\n",
      "|  b|       0.0|(2,[0],[1.0])|\n",
      "|  c|       2.0|    (2,[],[])|\n",
      "+---+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OneHotEncoder(inputCol='indexed_x1',outputCol='encoded_x1').transform(df_x1_indexed).show()\n",
    "#Converts categories to sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+\n",
      "| x1|indexed_x1|   encoded_x1|\n",
      "+---+----------+-------------+\n",
      "|  a|       1.0|(3,[1],[1.0])|\n",
      "|  a|       1.0|(3,[1],[1.0])|\n",
      "|  b|       0.0|(3,[0],[1.0])|\n",
      "|  b|       0.0|(3,[0],[1.0])|\n",
      "|  b|       0.0|(3,[0],[1.0])|\n",
      "|  c|       2.0|(3,[2],[1.0])|\n",
      "+---+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OneHotEncoder(inputCol='indexed_x1',outputCol='encoded_x1',dropLast=False).transform(df_x1_indexed).show()\n",
    "#We have 3 dummy variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization from continous to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions we can use to split a continuous variable into categories:\n",
    "\n",
    "* `pyspark.ml.feature.Binarizer`: split a column of continuous features given a threshold\n",
    "* `pyspark.ml.feature.Bucktizer`: split a column of continuous features into categories given several breaking points.\n",
    "    + with n+1n+1 split points, there are n categories (buckets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                  x1|                x2|\n",
      "+--------------------+------------------+\n",
      "| 0.47143516373249306| 6.834629351721363|\n",
      "| -1.1909756947064645| 7.127020269829002|\n",
      "|  1.4327069684260973|3.7025075479039495|\n",
      "| -0.3126518960917129| 5.611961860656249|\n",
      "| -0.7205887333650116| 5.030831653078097|\n",
      "|  0.8871629403077386|0.1376844959068224|\n",
      "|  0.8595884137174165| 7.728266216123741|\n",
      "| -0.6365235044173491| 8.826411906361166|\n",
      "|0.015696372114428918| 3.648859839013723|\n",
      "| -2.2426849541854055| 6.153961784334937|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating example data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(seed=1234)\n",
    "pdf = pd.DataFrame({\n",
    "        'x1': np.random.randn(10),\n",
    "        'x2': np.random.rand(10)*10\n",
    "    })\n",
    "np.random.seed(seed=None)\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the column x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer, Bucketizer\n",
    "binarizer = Binarizer(threshold=0,inputCol='x1',outputCol='x1_new')\n",
    "temp=binarizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x1', 'double'), ('x2', 'double'), ('x1_new', 'double')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.dtypes #'double' is binary representation of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketize the column x2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------+\n",
      "|                  x1|                x2|x2_new|\n",
      "+--------------------+------------------+------+\n",
      "| 0.47143516373249306| 6.834629351721363|   2.0|\n",
      "| -1.1909756947064645| 7.127020269829002|   2.0|\n",
      "|  1.4327069684260973|3.7025075479039495|   1.0|\n",
      "| -0.3126518960917129| 5.611961860656249|   2.0|\n",
      "| -0.7205887333650116| 5.030831653078097|   2.0|\n",
      "|  0.8871629403077386|0.1376844959068224|   0.0|\n",
      "|  0.8595884137174165| 7.728266216123741|   3.0|\n",
      "| -0.6365235044173491| 8.826411906361166|   3.0|\n",
      "|0.015696372114428918| 3.648859839013723|   1.0|\n",
      "| -2.2426849541854055| 6.153961784334937|   2.0|\n",
      "+--------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketizer=Bucketizer(splits=[0,2.5,5,7.5,10],inputCol='x2',outputCol='x2_new')\n",
    "bucketizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining it all together with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------+------+\n",
      "|                  x1|                x2|x1_new|x2_new|\n",
      "+--------------------+------------------+------+------+\n",
      "| 0.47143516373249306| 6.834629351721363|   1.0|   2.0|\n",
      "| -1.1909756947064645| 7.127020269829002|   0.0|   2.0|\n",
      "|  1.4327069684260973|3.7025075479039495|   1.0|   1.0|\n",
      "| -0.3126518960917129| 5.611961860656249|   0.0|   2.0|\n",
      "| -0.7205887333650116| 5.030831653078097|   0.0|   2.0|\n",
      "|  0.8871629403077386|0.1376844959068224|   1.0|   0.0|\n",
      "|  0.8595884137174165| 7.728266216123741|   1.0|   3.0|\n",
      "| -0.6365235044173491| 8.826411906361166|   0.0|   3.0|\n",
      "|0.015696372114428918| 3.648859839013723|   1.0|   1.0|\n",
      "| -2.2426849541854055| 6.153961784334937|   0.0|   2.0|\n",
      "+--------------------+------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Combine binarizer and bucketizer into one dataframe\n",
    "#Must use ml library with Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "stages=[binarizer,bucketizer]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "#transform the data\n",
    "pipeline.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import/Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `spark.read.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export data \n",
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "#Convert spark dataframe to pandas\n",
    "df=df.coalesce(numPartitions=1)\n",
    "#mt_cars.write.csv('saved_mtcars',header=True) does not work without Hadoop binary file \n",
    "df.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                  x1|                x2|\n",
      "+--------------------+------------------+\n",
      "| 0.47143516373249306| 6.834629351721363|\n",
      "| -1.1909756947064645| 7.127020269829002|\n",
      "|  1.4327069684260973|3.7025075479039495|\n",
      "| -0.3126518960917129| 5.611961860656249|\n",
      "| -0.7205887333650116| 5.030831653078097|\n",
      "|  0.8871629403077386|0.1376844959068224|\n",
      "|  0.8595884137174165| 7.728266216123741|\n",
      "| -0.6365235044173491| 8.826411906361166|\n",
      "|0.015696372114428918| 3.648859839013723|\n",
      "| -2.2426849541854055| 6.153961784334937|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read non-tabular data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
