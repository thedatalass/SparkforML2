{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib\n",
    "MLlib stands for Machine Learning Library. MLlib is now in a maintenance mode, that is, it is not actively being developed  \n",
    "  \n",
    "Starting with Spark 2.0, ML is the main machine learning library that operates on DataFrames instead of RDDs as is the case for MLlib RDD-base API. After reaching feature parity (roughly estimated for Spark 2.3), the RDD-based API will be deprecated and is expected to be removed in Spark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding ML packages in Spark\n",
    "* spark.mllib contains the original API built on top of RDDs.\n",
    "* spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional approach to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master='local[2]',\n",
    "               appName='my-spark')\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad=spark.read.csv('Advertising.csv',header=True,inferSchema=True)\n",
    "ad.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "|  [8.7,48.9,75.0]|  7.2|\n",
      "| [57.5,32.8,23.5]| 11.8|\n",
      "|[120.2,19.6,11.6]| 13.2|\n",
      "|    [8.6,2.1,1.0]|  4.8|\n",
      "| [199.8,2.6,21.2]| 10.6|\n",
      "|  [66.1,5.8,24.2]|  8.6|\n",
      "| [214.7,24.0,4.0]| 17.4|\n",
      "| [23.8,35.1,65.9]|  9.2|\n",
      "|   [97.5,7.6,7.2]|  9.7|\n",
      "|[204.1,32.9,46.0]| 19.0|\n",
      "|[195.4,47.7,52.9]| 22.4|\n",
      "|[67.8,36.6,114.0]| 12.5|\n",
      "|[281.4,39.6,55.8]| 24.4|\n",
      "| [69.2,20.5,18.3]| 11.3|\n",
      "|[147.3,23.9,19.1]| 14.6|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Features are 'TV', 'Radio', 'Newspaper', 'Sales' columns in the dataframe\n",
    "from pyspark.ml.linalg import Vectors\n",
    "ad_df=ad.rdd.map(lambda x: [Vectors.dense(x[0:3]),x[-1]]).toDF(['features','label'])\n",
    "ad_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr=LinearRegression(featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model=lr.fit(ad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+------------------+\n",
      "|         features|label|        prediction|\n",
      "+-----------------+-----+------------------+\n",
      "|[230.1,37.8,69.2]| 22.1| 20.52397440971517|\n",
      "| [44.5,39.3,45.1]| 10.4|12.337854820894362|\n",
      "| [17.2,45.9,69.3]|  9.3|12.307670779994238|\n",
      "|[151.5,41.3,58.5]| 18.5| 17.59782951168913|\n",
      "|[180.8,10.8,58.4]| 12.9|13.188671856831299|\n",
      "+-----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=lr_model.transform(ad_df)\n",
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897210638178952"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(predictionCol='prediction',labelCol='label')\n",
    "evaluator.setMetricName('r2').evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear regresiion with cross validation and tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test=ad_df.randomSplit([.8, .2],seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [0.7,39.6,8.7]|  1.6|\n",
      "| [7.8,38.9,50.6]|  6.6|\n",
      "|  [8.4,27.2,2.1]|  5.7|\n",
      "|   [8.6,2.1,1.0]|  4.8|\n",
      "| [8.7,48.9,75.0]|  7.2|\n",
      "| [13.1,0.4,25.6]|  5.3|\n",
      "|[16.9,43.7,89.4]|  8.7|\n",
      "| [17.2,4.1,31.6]|  5.9|\n",
      "|[17.2,45.9,69.3]|  9.3|\n",
      "|[17.9,37.6,21.6]|  8.0|\n",
      "|[18.7,12.1,23.4]|  6.7|\n",
      "|[19.4,16.0,22.3]|  6.6|\n",
      "|[19.6,20.1,17.0]|  7.6|\n",
      "|[23.8,35.1,65.9]|  9.2|\n",
      "|[25.0,11.0,29.7]|  7.2|\n",
      "|[25.1,25.7,43.3]|  8.5|\n",
      "| [25.6,39.0,9.3]|  9.5|\n",
      "|[26.8,33.0,19.3]|  8.8|\n",
      "| [28.6,1.5,33.0]|  7.3|\n",
      "| [31.5,24.6,2.2]|  9.5|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=LinearRegression(featuresCol='features',labelCol='label')\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid=ParamGridBuilder().\\\n",
    "            addGrid(lr.regParam, [0.0, .5, 1.0]).\\\n",
    "            addGrid(lr.elasticNetParam, [0.2,.5,.8]).\\\n",
    "        build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator=RegressionEvaluator(predictionCol='prediction',labelCol='label',metricName='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv=CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_model=cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_training=cv_model.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test_cv=cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8982486958337326"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('r2').evaluate(pred_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8896562076565583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('r2').evaluate(pred_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.046507497430939085,0.18085452246520456,-0.0010752054907401342]\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.075068686285647\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing categorical columns\n",
    "Using pipeline we will perform following transformations on categorical data:\n",
    "\n",
    "* StringIndexer identify column as categorical variable or if want to convert the textual data to numeric data keeping the categorical context\n",
    "* OneHotEncoder Binarizing categorical index in the form appropriate for modelling\n",
    "* VectorAssembler all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build cross-validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---+\n",
      "|age|education|wantsMore|  y|\n",
      "+---+---------+---------+---+\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "+---+---------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuse=spark.read.csv('cuse_binary.csv',header=True,inferSchema=True)\n",
    "cuse.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'education', 'wantsMore']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# categorical columns\n",
    "categorical_columns = cuse.columns[0:3]\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build StringIndexer stages\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c) for c in categorical_columns]\n",
    "# encode label column and add it to stringindexer_stages\n",
    "stringindexer_stages += [StringIndexer(inputCol='y', outputCol='label')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build OneHotEncoder stages\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build VectorAssembler stage\n",
    "feature_columns = ['onehot_' + c for c in categorical_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=feature_columns, outputCol='features') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining all stages into pipeling model\n",
    "all_stages = stringindexer_stages + onehotencoder_stages + [vectorassembler_stage]\n",
    "pipeline = Pipeline(stages=all_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit pipeline model\n",
    "pipeline_model = pipeline.fit(cuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------------+-------------------+-----+\n",
      "|   onehot_age|onehot_education|onehot_wantsMore|           features|label|\n",
      "+-------------+----------------+----------------+-------------------+-----+\n",
      "|(3,[2],[1.0])|       (1,[],[])|   (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|  0.0|\n",
      "|(3,[2],[1.0])|       (1,[],[])|   (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|  0.0|\n",
      "|(3,[2],[1.0])|       (1,[],[])|   (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|  0.0|\n",
      "|(3,[2],[1.0])|       (1,[],[])|   (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|  0.0|\n",
      "|(3,[2],[1.0])|       (1,[],[])|   (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|  0.0|\n",
      "+-------------+----------------+----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform data\n",
    "final_columns = feature_columns + ['features', 'label']\n",
    "cuse_df = pipeline_model.transform(cuse).\\\n",
    "            select(final_columns)\n",
    "            \n",
    "cuse_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test datasets\n",
    "training, test = cuse_df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter grid to tune model\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(logr.regParam, [0.0, 0.5, 1.0, 2.0]).\\\n",
    "    addGrid(logr.elasticNetParam, [0.0, 0.5, 1.0]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=logr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit cross-validation model\n",
    "cv_model = cv.fit(cuse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_columns = ['label', 'prediction', 'rawPrediction', 'probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction                             |probability                            |\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_training_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction                             |probability                            |\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|0.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "|1.0  |1.0       |[-0.05602431718564116,0.05602431718564116]|[0.4859975829890087,0.5140024170109914]|\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "pred_test_cv = cv_model.transform(test)\n",
    "pred_test_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Decision tree with cross-validation and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt=DecisionTreeClassifier(featuresCol='features',labelCol='label')\n",
    "# Parameter grid to tune model\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2, 5, 6, 8]).\\\n",
    "    build()\n",
    "# Evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName='areaUnderROC')\n",
    "# Cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "cv_model=cv.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction|probability                            |\n",
      "+-----+----------+-------------+---------------------------------------+\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "+-----+----------+-------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_training_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction|probability                            |\n",
      "+-----+----------+-------------+---------------------------------------+\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|0.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "|1.0  |0.0       |[227.0,211.0]|[0.5182648401826484,0.4817351598173516]|\n",
      "+-----+----------+-------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "pred_test_cv = cv_model.transform(test)\n",
    "pred_test_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(label=0.0, prediction=0.0): 216,\n",
       "             Row(label=1.0, prediction=0.0): 101})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_and_pred=cv_model.transform(test).select('label','prediction')\n",
    "label_and_pred.rdd.zipWithIndex().countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining classifier with cross-validation and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "dt=GBTClassifier(featuresCol='features',labelCol='label')\n",
    "# Parameter grid to tune model\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2, 5, 6, 8]).\\\n",
    "    addGrid(dt.stepSize, [0.01,.1,.2]).\\\n",
    "    build()\n",
    "# Evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName='areaUnderROC')\n",
    "# Cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "cv_model=cv.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction                             |probability                            |\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_training_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|label|prediction|rawPrediction                             |probability                            |\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|0.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "|1.0  |1.0       |[-0.04903348521826213,0.04903348521826213]|[0.4755028869062719,0.5244971130937282]|\n",
      "+-----+----------+------------------------------------------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test data\n",
    "pred_test_cv = cv_model.transform(test)\n",
    "pred_test_cv.select(show_columns).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6798976885183781"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('areaUnderROC').evaluate(pred_training_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.687935460212688"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('areaUnderROC').evaluate(pred_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel._java_obj.getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel._java_obj.getStepSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a problem\n",
    "\n",
    "We want to develop a movie recommender system with Spark on data from MovieLens. MovieLens is a project developed by GroupLens, a research laboratory at the University of Minnesota. MovieLens provides an online movie recommender application that uses anonymously-collected data to improve recommender algorithms. \n",
    "\n",
    "To help people develop the best recommendation algorithms, MovieLens also released several data sets: \n",
    "\n",
    "* full data set consists of more than 24 million ratings across more than 40,000 movies by more than 250,000 users.\n",
    "\n",
    "* small data set that is a subset of the full data set. \n",
    "\n",
    "In our example we will start building a working program with a small data set to get faster performance while interacting, exploring, and getting errors with your data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|   450| 148372|   5.0|1475737053|\n",
      "|   270|  76093|   3.0|1469306154|\n",
      "|   625| 110102|   4.0|1452853116|\n",
      "|   195|    112|   1.0| 977724281|\n",
      "+------+-------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings=spark.read.csv('ml-latest-small/ratings.csv',header=True,inferSchema=True).repartition(4).cache()\n",
    "ratings.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+------------------------------------+\n",
      "|movieId|title                                    |genres                              |\n",
      "+-------+-----------------------------------------+------------------------------------+\n",
      "|8754   |Prime of Miss Jean Brodie, The (1969)    |Drama                               |\n",
      "|111486 |Lesson of the Evil (Aku no kyôten) (2012)|Thriller                            |\n",
      "|1033   |Fox and the Hound, The (1981)            |Animation|Children|Drama            |\n",
      "|6536   |Sinbad: Legend of the Seven Seas (2003)  |Adventure|Animation|Children|Fantasy|\n",
      "+-------+-----------------------------------------+------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies=spark.read.csv('ml-latest-small/movies.csv',header=True,inferSchema=True).repartition(4).cache()\n",
    "movies.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|summary|           userId|           movieId|            rating|           timestamp|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|  count|           100004|            100004|            100004|              100004|\n",
      "|   mean|347.0113095476181|12548.664363425463| 3.543608255669773|1.1296390869392424E9|\n",
      "| stddev|195.1638379781956| 26369.19896881519|1.0580641091070395|1.9168582602710992E8|\n",
      "|    min|                1|                 1|               0.5|           789652009|\n",
      "|    max|              671|            163949|               5.0|          1476640644|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1959"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.filter('rating==1').select('movieId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|loosers|\n",
      "+-------+\n",
      "|   1959|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.createOrReplaceTempView('ratings')\n",
    "spark.sql(\"SELECT COUNT(DISTINCT(movieId)) AS loosers FROM ratings WHERE rating==1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendation system\n",
    "\n",
    "Recommender systems is a family of methods that enable filtering through large observation and information space in order to provide recommendations in the information space that user does not have any observation, where the information space is all of the available items that user could choose or select and observation space is what user experienced or observed so far.\n",
    "\n",
    "\n",
    "There are different methods for building a recommender system, such as, user-based, content-based, or collaborative filtering. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering\n",
    "\n",
    "\n",
    "#### Advantages over content based methods\n",
    "\n",
    "#### Disadvantages over content based methods¶\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternating Least Squares Algorithm\n",
    "\n",
    "We have users $u$ for items $i$ matrix as in the following:\n",
    "$$Q_{ui} = \\cases{\n",
    "r  & \\text{if user u rate item i} \\cr\n",
    "0 & \\text{if user u did not rate item i}\n",
    "} $$\n",
    "\n",
    "where $r$ is what rating values can be. If we have $m$ users and $n$ items, then we want to learn a matrix of factors which represent movies. That is, the factor vector for each movie and that would be how we represent the movie in the feature space. Note that, we do not have any knowledge of the category of the movie at this point. We also want to learn a factor vector for each user in a similar way how we represent the movie. Factor matrix for movies $X \\in \\mathbb{R}^{mxf}$ and factor matrix(each movie is a column vector) for users $X \\in \\mathbb{R}^{mxf}$(each user is a row vector). However, we have two unknown variables. Therefore, we will adopt an alternating least squares approach with regularization. By doing so, we first estimate $Y$ using $X$ and estimate $X$ by using $Y$. After enough number of iterations, we are aiming to reach a convergence point where either the matrices $X$ and $Y$ are no longer changing or the change is quite small. However, there is a small problem in the data. We have neither user full data nor full items data, (suprisingly) this is also why we are trying to build the recommendation engine in the first place. Therefore, we may want to penalize the movies that do not have ratings in the update rule. By doing so, we will depend on only the movies that have ratings from the users and do not make any assumption around the movies that are not rated in the recommendation. Let's call this weight matrix $w_{ui}$ as such:\n",
    "$$w_{ui} = \\cases{\n",
    "0 &\\text{if  } q_{ui} = 0 \\cr\n",
    "1 & \\text{ else} \n",
    "}$$\n",
    "Then, cost functions that we are trying to minimize is in the following:\n",
    "$$J(x_u) = (q_u - x_u Y) W_u (q_u - x_u Y)^T + \\lambda x_u x_u^T$$\n",
    "$$J(y_i) = (q_i - X y_i) W_i (q_i - X y_i)^T + \\lambda y_i y_i^T$$\n",
    "\n",
    "Note that we need regularization terms in order to avoid the overfitting the data. Ideally, regularization parameters need to be tuned using cross-validation in the dataset for algorithm to generalize better. In this post, I will use the whole dataset. Solutions for factor vectors are given as follows:\n",
    "$$x_u = (Y W_u Y^T + \\lambda I)^{-1} Y W_u q_u$$\n",
    "$$y_i = (X^T Wi X + \\lambda I)^{-1} X^T W_i q_i$$\n",
    "\n",
    "where $W_u \\in \\mathbb{R}^{nxn}$ and $W_u \\in \\mathbb{R}^{mxm}$ diagonal matrices. The algorithm is pretty much of it. In the regulaization, we may want to incorporate both factor matrices in the update rules as well if we want to be more restrictive. That may generalize better, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the recommender system\n",
    "\n",
    "In our example, the ALS algorithm will create a matrix of all users versus all movies. Most cells in the matrix will be empty. An empty cell means the user hasn't reviewed the movie yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for movie factors and solving for user factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "model=ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\").fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|   575|    148|   4.0|1012605106|  3.925006|\n",
      "|   232|    463|   4.0| 955089443|  3.840804|\n",
      "|   452|    463|   2.0| 976424451| 2.5061953|\n",
      "|   380|    463|   3.0| 968949106| 2.9723182|\n",
      "|   534|    463|   4.0| 973377486|  3.884037|\n",
      "|   242|    463|   4.0| 956685706| 3.8402894|\n",
      "|    30|    463|   4.0| 945277405| 3.5451906|\n",
      "|   311|    463|   3.0| 898008246| 2.9873717|\n",
      "|    85|    471|   3.0| 837512312|  2.872746|\n",
      "|   588|    471|   3.0| 842298526| 3.6434338|\n",
      "|   126|    471|   5.0| 833287141| 3.9697504|\n",
      "|   460|    471|   5.0|1072836030|  4.082004|\n",
      "|   350|    471|   3.0|1011714986| 3.5613809|\n",
      "|   548|    471|   4.0| 857407799| 3.4402337|\n",
      "|   602|    471|   3.0| 842357922| 4.1293125|\n",
      "|   285|    471|   5.0| 965092130|  3.846668|\n",
      "|   274|    471|   5.0|1074104142| 3.8721604|\n",
      "|   440|    471|   3.0| 835337519| 3.3404179|\n",
      "|    86|    471|   4.0| 848161161|   4.14771|\n",
      "|   292|    471|   3.5|1140049920|  3.879138|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=model.transform(ratings)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Evaluation on training data\n",
    "We need to evaluate the performance of the model by comparing the predicted values with the original values. In this case of numeric forecasting RMSE is a good choice as a measure of fit. Use the RegressionEvaluator method to compare continuous values with the root mean squared calculation. The root mean squared error (RMSE) calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6121762502409693"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test=ratings.randomSplit([80.0,20.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|   380|    463|   3.0| 968949106|  2.810031|\n",
      "|   534|    463|   4.0| 973377486| 3.5423799|\n",
      "|    30|    463|   4.0| 945277405| 3.1847305|\n",
      "|    85|    471|   3.0| 837512312|  3.182696|\n",
      "|   274|    471|   5.0|1074104142| 3.3121417|\n",
      "|   440|    471|   3.0| 835337519| 3.6286314|\n",
      "|   491|    471|   3.0| 940797129| 4.0265474|\n",
      "|   452|    471|   3.0| 976422396| 3.4104712|\n",
      "|    92|    471|   4.0| 848526594|  4.034794|\n",
      "|   607|    471|   4.0|1118247731| 3.7384584|\n",
      "|   358|    471|   5.0| 957479605| 3.9340591|\n",
      "|   502|    471|   4.0| 861322541|  4.155453|\n",
      "|   514|    471|   4.0| 853893788| 4.2163954|\n",
      "|   195|    471|   3.0| 976289176| 3.5753496|\n",
      "|    30|    471|   4.0| 945112993| 3.5556245|\n",
      "|   399|    471|   5.0| 841562601| 3.4645963|\n",
      "|   509|    496|   3.0| 940013481|  2.124097|\n",
      "|   133|   1088|   1.5|1416166508| 2.3406138|\n",
      "|   111|   1088|   3.5|1097431651| 3.3382437|\n",
      "|   607|   1088|   2.0|1148586162| 3.4607801|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\").fit(training)\n",
    "predictions=model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Evaluation on test data\n",
    "To find out how our algorithm works on unseen cases we need to split  ratings data set between training test data set. Then rerun the steps to train the model on the training set, run it on the test set, and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling NaN results\n",
    "A NaN result is due to SPARK-14489 issue and because the model can't predict values for users for which there's no data\n",
    "\n",
    "This typically occurs in two scenarios:\n",
    "\n",
    "* In production, for new users or items that have no rating history and on which the model has not been trained (this is the “cold start problem”).\n",
    "* During cross-validation, the data is split between training and evaluation sets. When using simple random splits as in Spark’s CrossValidator or TrainValidationSplit, it is actually very common to encounter users and/or items in the evaluation set that are not in the training set\n",
    "\n",
    "By default, Spark assigns NaN predictions during ALSModel.transform when a user and/or item factor is not present in the model. This can be useful in a production system, since it indicates a new user or item, and so the system can make a decision on some fallback to use as the prediction.\n",
    "\n",
    "During cross-validation this is not acceptable, since any NaN predicted values will result in NaN results for the evaluation metric and makes model selection impossible.\n",
    "\n",
    "Spark allows users to set the coldStartStrategy parameter to “drop” in order to drop any rows in the DataFrame of predictions that contain NaN values. The evaluation metric will then be computed over the non-NaN data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|   380|    463|   3.0| 968949106|  2.810031|\n",
      "|   534|    463|   4.0| 973377486| 3.5423799|\n",
      "|    30|    463|   4.0| 945277405| 3.1847305|\n",
      "|    85|    471|   3.0| 837512312|  3.182696|\n",
      "|   274|    471|   5.0|1074104142| 3.3121417|\n",
      "|   440|    471|   3.0| 835337519| 3.6286314|\n",
      "|   491|    471|   3.0| 940797129| 4.0265474|\n",
      "|   452|    471|   3.0| 976422396| 3.4104712|\n",
      "|    92|    471|   4.0| 848526594|  4.034794|\n",
      "|   607|    471|   4.0|1118247731| 3.7384584|\n",
      "|   358|    471|   5.0| 957479605| 3.9340591|\n",
      "|   502|    471|   4.0| 861322541|  4.155453|\n",
      "|   514|    471|   4.0| 853893788| 4.2163954|\n",
      "|   195|    471|   3.0| 976289176| 3.5753496|\n",
      "|    30|    471|   4.0| 945112993| 3.5556245|\n",
      "|   399|    471|   5.0| 841562601| 3.4645963|\n",
      "|   509|    496|   3.0| 940013481|  2.124097|\n",
      "|   133|   1088|   1.5|1416166508| 2.3406138|\n",
      "|   111|   1088|   3.5|1097431651| 3.3382437|\n",
      "|   607|   1088|   2.0|1148586162| 3.4607801|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.918654025287006"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\",coldStartStrategy=\"drop\").fit(training)\n",
    "predictions=model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning model \n",
    "\n",
    "What are the hyperparameters we can use to tune our ALS model?\n",
    "There are three such hyperparameters in our case:\n",
    "* rank = the number of latent factors in the model\n",
    "* maxIter = the maximum number of iterations\n",
    "* regParam = the regularization parameter\n",
    "\n",
    "To find the best values we can test several values for those hyperparameters and choose the best configuration.\n",
    "In the same way as before we will define a grid of parameter combinations and  run a grid search over the combinations to evaluate the resulting models and comparing their performance using Spark CrossValidator which facilitates multiple values for rank and regParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(model.rank, [8,10,12]).\\\n",
    "    build()\n",
    "\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "crossval=CrossValidator(estimator=model,estimatorParamMaps=param_grid,evaluator=evaluator,numFolds=2)\n",
    "cv_model=crossval.fit(training)\n",
    "\n",
    "\n",
    "predictions_train=cv_model.transform(training)\n",
    "predictions_test=cv_model.transform(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5601740747315376"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('rmse').evaluate(predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180136869757596"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('rmse').evaluate(predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment to recommend movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
